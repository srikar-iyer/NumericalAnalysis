{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Numerical Analysis HW5 - Srikar Iyer'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import companion\n",
    "from numpy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "'Numerical Analysis HW5 - Srikar Iyer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Question 3\n",
    "a: Implementation of the Power Method\n",
    "The power method works because given Ax = lambda(x), A^k x = lambda^k x implies that with multiple iterations,\n",
    "the dominant eigenvalue would influence A^k x the most, which would be k times the most dominant eigenvector.\n",
    "If we norm the this value (which would be A^k x), we can approximate the dominant eigenvector.\n",
    "'''\n",
    "def powermethod(A, x_k, num_iterations: int):\n",
    "    for _ in range(num_iterations):\n",
    "        # calculates the matrix-by-vector product Ax_k  for every iteration (starting with a given x_0), looped by num_iterations\n",
    "        x_k1 = np.dot(A, x_k)\n",
    "\n",
    "        # calculates the norm\n",
    "        x_k1_norm = np.linalg.norm(x_k1)\n",
    "\n",
    "        # re normalizes the vector\n",
    "        x_k = x_k1 / x_k1_norm\n",
    "\n",
    "    return x_k \n",
    "\n",
    "'''b: executes the power method I derived for a matrix  np.array([[-2, 1, -4], [1, 1, 1], [4, 1, -2]]) and vectors [1 2 -1], [1 2 1] for \n",
    "5 iterations. Then, we compute the actual numpy-derived eigenvalues and eigenvectors and explain what is happening.'''\n",
    "eval, evec = np.linalg.eig(np.array([[-2, 1, 4], [1, 1, 1], [4, 1, -2]]))\n",
    "print(\"Given the initial x_k = [1 2 -1], the first 5 iterates are: \")\n",
    "for i in range(5):\n",
    "    print(powermethod(np.array([[-2, 1, -4], [1, 1, 1], [4, 1, -2]]), [1, 2, -1], i))\n",
    "print('Given the initial x_k = [1 2 1], the first 5 iterates are: ')\n",
    "for i in range(5):\n",
    "    print(powermethod(np.array([[-2, 1, -4], [1, 1, 1], [4, 1, -2]]), [1, 2, 1], i))\n",
    "print(\"The eigenvalues of A are : \" + str(eval))\n",
    "print(\"The corresponding eigenvectors of A are\")\n",
    "print(str(evec))\n",
    "print(\"The power method with the initial vector [1 2 1] converged to the dominant eigenvector of A, roughly [0.02745888 -0.96430928 -0.26335074]\")\n",
    "print(\"However, the power method with the vector [1 2 -1] converged to a more degenerate eigenvector. This is because [1 2 -1] is in the nullspace of the column\")\n",
    "print(\"[-2 1 4] so when the power method was calculated, the leftmost column became zeros and the eigenvector was then iterated with the columns\")\n",
    "print(\"[1 1 1] and [-4 1 -2]. Hence, since the Ax spanned different dimensions for both x's, the limit as k-> infinity of the power method iteration\")\n",
    "print(\"of A_k wasn't the same for both initial vectors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "c: Implementation of the inverse method\n",
    "The inverse method works because, given a full rank, symmetric A, Ax=lambda(x) and (A-lambda * I)x = 0. So, given some initial vector x_k,\n",
    "the iteration of (A-lambda * I)x_k approximates (A-lambda * I)x where x_k is closest to the nearest eigenvalue x. When the error\n",
    "term alpha_j = beta_j (lambda_j - theta) is not 0 (beta is a constant and j represents a specific \n",
    "eigenvalue/eigenvector pair in A), the iteration continues. This works computationally\n",
    "because this error term modifies x_k to approximate x for each iteration, and even if \n",
    "it could equal 0 through manual computation, rounding errors ensure that this happensrarely.\n",
    "''' \n",
    "def inversemethod(A, x_k, theta: int, num_iterations: int):\n",
    "    for _ in range(num_iterations):\n",
    "        # calculates A-theta(I) for every iteration (starting with a given x_0), looped by num_iterations\n",
    "        x_k1 = (A - theta * np.ones(np.shape(A))).dot(x_k)\n",
    "        # calculates the norm\n",
    "        x_k1_norm = np.linalg.norm(x_k1)\n",
    "        # re normalizes the vector\n",
    "        x_k = x_k1 / x_k1_norm\n",
    "    return x_k \n",
    "'''d: executes the inverse method for the matrix  np.array([[-2, 1, -4], [1, 1, 1], [4, 1, -2]]) for \n",
    "500 iterations. We do this three times with 3 x_k's close to their respective eigenvectors and 3 theta's \n",
    "near the eigenvales in order to converge to the eigenvectors themselves.'''\n",
    "print('Given the initial x_k = [7, 7.5, -7], the first 5 iterates are: ')\n",
    "for i in range(5):\n",
    "    print(inversemethod(np.array([[-2, 1, 4], [1, 1, 1], [4, 1, -2]]), [0.7, 0.01, -0.7], -5.9, i))\n",
    "print('Given the initial x_k = [-6, -6, -6], the first 5 iterates are: ')\n",
    "for i in range(5):\n",
    "    print(inversemethod(np.array([[-2, 1, 4], [1, 1, 1], [4, 1, -2]]), [-0.6, -.6001, -.6], 2.9, i))\n",
    "print('Given the initial x_k = [0.4, -0.8, 0.4], the first 5 iterates are: ')\n",
    "for i in range(5):\n",
    "    print(inversemethod(np.array([[-2, 1, 4], [1, 1, 1], [4, 1, -2]]), [0.4, -0.822, 0.4], -0.01, i))\n",
    "print(\"If we look at the 500th iteration, the inverse iteration for the 3 vectors converges clearly to 3 different eigenvalues.\")\n",
    "print(inversemethod(np.array([[-2, 1, 4], [1, 1, 1], [4, 1, -2]]), [0.7, 0.01, -0.7], -5.9, 500))\n",
    "print(inversemethod(np.array([[-2, 1, 4], [1, 1, 1], [4, 1, -2]]), [-0.6, -.6001, -.6], 2.9, 500))\n",
    "print(inversemethod(np.array([[-2, 1, 4], [1, 1, 1], [4, 1, -2]]), [0.4, -0.822, 0.4], -0.01, 500))\n",
    "print(\"The sequence for these 3 vectors converge to these eigenvectors since each vector is very close to each eigenvector, and \")\n",
    "print(\"each theta was close to each corresponding eigenvector. Since the iteration modifies the approximation closer to the \")\n",
    "print(\"corresponding eigenvector, it makes sense that the convergence matches the intuition.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''5b: We find the error of 2 root finding methods  of Wilkinson's Polynomial of degree 15. \n",
    "First, we create a characteristic matrix that we can find the eigenvalues of, so we need the coefficients.\n",
    "'''\n",
    "coeff = np.poly([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])\n",
    "mat = companion(coeff)\n",
    "print(coeff), print(np.roots(coeff)),print(mat), print(np.linalg.eig(mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f487cd0f1b314ae66fc495c393638a2de5bc0981475fb8126a23876567974a5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
